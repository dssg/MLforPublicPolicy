{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Naive Bayes and Random Forests\n",
    "In this lab we'll take a closer look the [Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) and [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes is famously effective for spam email classification. We'll try our hand at applying NB on the [Spam email dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI data repository. Download the dataset and move it to your lab data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/SMSSpamCollection'\n",
    "df = pd.read_table(fname, names=['class', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "Given collection of documents $d^{(1)}, d^{(2)}, ..., d^{(N)}$ with corresponding labels $y^{(i)}$, we first need to extract features from each document. We'll do this by computing word counts of each word in each document. This is commonly referred to as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation.\n",
    "\n",
    "After doing some preprocessing of our data, we'll have: $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, where:\n",
    "* $x^{(i)} \\in R^s$ is a word count feature vector for document $i$, and $s$ is the size of the vocabulary. For the purposes of this lab, we define our vocabulary to be the set of words that appear in our dataset of documents.\n",
    "$$ x^{(i)} = (\\theta^{(i)}_1, \\theta^{(i)}_2, ..., \\theta^{(i)}_s)$$\n",
    "$$\\theta^{(i)}_k = \\text{ the number of times word $k$ appears in document $i$}$$\n",
    "\n",
    "\n",
    "* $y^{(i)} \\in \\{0, 1\\}$ is its class. In the spam email classification task, 0 denotes spam, 1 denotes ham.\n",
    "\n",
    "Before we can use sklearn's Naive Bayes classifier, we need to compute word count feature vectors. We can do this manually like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def get_all_words(docs):\n",
    "    '''\n",
    "    docs: pandas.Series of strings\n",
    "    Returns: set of words(strings)\n",
    "    '''\n",
    "    # Compute all the words in the docs\n",
    "    words = set()\n",
    "    for d in docs:\n",
    "        # this will strip punctuation\n",
    "        # ref: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        d = d.translate(None, string.punctuation)\n",
    "        words = words.union(set(d.split()))\n",
    "    return words\n",
    "\n",
    "def make_word_cnt_feats(docs, vocabulary=None):\n",
    "    '''\n",
    "    docs: iterable(ie: list or pandas.Series) of strings\n",
    "    vocabulary: set of words(strings) that appear in docs\n",
    "    Returns: numpy matrix of the word count features\n",
    "    '''\n",
    "    if vocabulary is None:\n",
    "        vocabulary = get_all_words(docs)\n",
    "    words = sorted(vocabulary)\n",
    "    word_dict = {w: idx for idx, w in enumerate(words)}\n",
    "    \n",
    "    # feature matrix will be of size n x (size of vocabulary + 1)\n",
    "    # add 1 to account for words that dont appear in the vocabulary to avoid random issues\n",
    "    vocab_size = len(words)\n",
    "    word_cnt_features = np.zeros((len(docs), len(words) + 1))\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx]\n",
    "        doc = doc.translate(None, string.punctuation)\n",
    "        words = doc.split()\n",
    "        for w in words:\n",
    "            word_idx = word_dict.get(w, vocab_size)\n",
    "            word_cnt_features[idx, word_idx] += 1\n",
    "            \n",
    "    return word_cnt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5572, 11748)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_all_words(df['text'])\n",
    "feature_matrix = make_word_cnt_feats(df['text'], vocabulary)\n",
    "print('Feature matrix shape: {}'.format(feature_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that sklearn actually has a built-in method to do this kind of word count featurization! (But of course, it's good to get your hands dirty with manual featurization from time to time to understand all the ingredients of these algorithms). See sklearn's [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function for more details.                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "word_cnt_feats = count_vectorizer.fit_transform(df['text'])\n",
    "word_cnt_feats\n",
    "# looks like this gives us fewer columns, which means we didn't prune\n",
    "# a bunch of unwanted terms(maybe words with numbers?) in our code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) has a bunch of useful functions that you should checkout. The important ones that we'll use are:\n",
    "* [fit](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit): this learns the vocabulary of the input collection of strings/\"documents\"\n",
    "* [transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform): given a collection of documents, this produces the word count features for the documents\n",
    "* fit_transform: calls fit, then transform\n",
    "\n",
    "Note that the word count feature matrix(aka document-term matrix) is in sparse format. This is not an issue for the sklearn models. But if you want to expand them to non-sparse numpy arrays, use the toarray function. Note that this can be problematic for bigger datasets! We can somewhat get around this b/c this dataset is small enough that we can fully expand a 5k by 8k matrix(since it's sparse enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = word_cnt_feats.toarray()\n",
    "Y = (df['class'] == 'ham')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the document-term matrix format, we can use the Naive Bayes classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.983\n",
      "F1 Score: 0.991\n",
      "ROC AUC Score: 0.962\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}'.format(accuracy_score(preds, y_test)))\n",
    "print('F1 Score: {:.3f}'.format(f1_score(preds, y_test)))\n",
    "print('ROC AUC Score: {:.3f}'.format(roc_auc_score(preds, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Tasks\n",
    "Now it's your turn to use it on the Naive Bayes model. Pick an evaluation metric to use for cross validation like precision, recall, F1 score, ROC AUC score, etc. In the spam classification setting, do you think it makes sense to optimize for higher precision? recall? f1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Note that the code above fixed alpha to 1. But as we've learned in class and last week's lab, we should use cross validation for any kind of parameter search. Use [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to do the cross validation over 5 folds to find the alpha that works best on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "alphas = [1, 5, 10, 25, 100] # you should vary these \n",
    "cv_split_results = {} # store your cross validation results\n",
    "kf = KFold(n_splits=5)\n",
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Now that you've found the optimal alpha using cross validation over the training set, train a Naive Bayes model using the optimal alpha over the entirety of the training data and evaluate this model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Questions:\n",
    "* Naive Bayes has a seemingly stringent independence assumption. Do you think the variables in this email classification problem(word count vectors) are really independent? Does this matter? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Two weeks ago, we played around with decision trees. In general, practictioners almost never use decision trees over Random Forests. Recall from lecture, that Random Forests takes some number of subsamples of the data:\n",
    "$S_1, S_2, ..., S_k \\subseteq \\{(x^{(i)}, y^{(i)}\\}_{i=0}^N$. On each subsample of the data, $S_i$, the Random Forests algorithm trains a decision tree on $S_i$.\n",
    "\n",
    "At classification time, when we get a datapoint $x$, the Random Forests classifier runs $x$ through decision trees $D_1, D_2, ..., D_k$, which produces a \"probability\" estimate of each class:\n",
    "\n",
    "$$P(y = 1 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "$$P(y = 0 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "Then from there, we get a class prediction by picking $y = 1$ if $P(y = 1 | x) > T$, for some threshold probability $T$.\n",
    "\n",
    "\n",
    "Instead of using the spam dataset to try out Random Forests, we'll re-use the credit loan data from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever data cleaning, data imputing, etc you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Parameters\n",
    "\n",
    "The Random Forest parameters are more or less the same as those for Decision Trees. The only additional parameter is n_estimators, which is the number of decision trees to grow for the algorithm.\n",
    "* n_estimators : integer, the number of trees in the forest.\n",
    "* criterion : string,  “gini” or “entropy” \n",
    "* max_features : int, float, string or None, the number of features to consider when looking for the best split\n",
    "* max_depth : integer, the maximum depth of the tree\n",
    "* min_samples_split : int, float, the minimum number of samples required to split an internal node:\n",
    "* min_samples_leaf : int, float, optional (default=1), the minimum number of samples required to be at a leaf node:\n",
    "* min_weight_fraction_leaf : float, optional \n",
    "* max_leaf_nodes : int or None, optional (default=None)\n",
    "\n",
    "0) What do you think will be the effect of increasing the n\\_estimators parameter? If n_estimators = 1, is this any different to using a decision tree classifier(assuming you set all the other parameters like min_sample_split, max_depth, etc the same way in both cases)?\n",
    "\n",
    "1) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Pick a few values for each of the random forest parameters for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param settings to try out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last weeks lab, we saw how to grid search over all possible model parameters and cross validation folds. To refresh your memory, below is a slightly updated version of that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of a manual grid search for LogisticRegression on random data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def sample_manual_grid_search(n_samples, n_feats, splits):\n",
    "    # make the fake data\n",
    "    X = np.random.randint(0, 10, (n_samples, n_feats))\n",
    "    Y = (np.random.random(n_samples) > 0.5) # random true/false\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "    \n",
    "    # params to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    rows = [] # store dicts of the cross validation results for each fold\n",
    "    for p in penalties:\n",
    "        for c in c_vals:\n",
    "\n",
    "            fold_dict = {\n",
    "                    # NOTE: these key values must be exactly as they're spelled in\n",
    "                    # the sklearn model!\n",
    "                    'penalty': p,\n",
    "                    'C': c,\n",
    "            }\n",
    "            fold_results = {}\n",
    "            for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "                x_split_train, x_split_test = X_train[train_idx], X_train[test_idx]\n",
    "                y_split_train, y_split_test = y_train[train_idx], y_train[test_idx]\n",
    "                    \n",
    "                model = LogisticRegression(**fold_dict)\n",
    "                model.fit(x_split_train, y_split_train)\n",
    "                pred_probs = model.predict_proba(x_split_test)\n",
    "                loss = roc_auc_score(y_split_test, pred_probs[:, 0])\n",
    "                \n",
    "                # Keep track of the performance of the model on each fold and store the result\n",
    "                fold_results['fold_{}'.format(fold_num+1)] = loss\n",
    "                    \n",
    "            # now that we have all the fold results, update the dict containing the model params\n",
    "            # with the performance results\n",
    "            fold_dict.update(fold_results)\n",
    "            fold_dict['avg_score'] = np.mean(fold_results.values())\n",
    "            rows.append(fold_dict)\n",
    "\n",
    "    # we can create a dataframe from a list of dicts. This produces a dataframe whose columns are the keys of\n",
    "    # the dicts in the list. Recall that rows is a list of dicts\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results for sample data with Logistic Regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>fold_1</th>\n",
       "      <th>fold_2</th>\n",
       "      <th>fold_3</th>\n",
       "      <th>fold_4</th>\n",
       "      <th>fold_5</th>\n",
       "      <th>penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.635608</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.633016</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.606931</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.550582</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  avg_score    fold_1    fold_2    fold_3    fold_4    fold_5 penalty\n",
       "0    0.01   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000      l1\n",
       "1    0.10   0.635608  0.314286  0.740741  0.694444  0.657143  0.771429      l1\n",
       "2    1.00   0.633016  0.514286  0.666667  0.555556  0.685714  0.742857      l1\n",
       "3   10.00   0.606931  0.600000  0.592593  0.527778  0.542857  0.771429      l1\n",
       "4  100.00   0.550582  0.571429  0.481481  0.500000  0.542857  0.657143      l1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = sample_manual_grid_search(n_samples=100, n_feats=20, splits=5)\n",
    "print(\"Grid Search Results for sample data with Logistic Regression:\")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Do the same kind of grid search cross validation for Random Forests classifier. Which set of parameters gets the best average score over all folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your grid search CV for RandomForests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation\n",
    "Writing 8x nested for-loops to search over all parameters can be a bit cumbersome. Luckily, sklearn has a very nice [GridSearchCV class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) that can do all this grid searching and cross validating for us!\n",
    "\n",
    "GridSearchCV requires:\n",
    "* an estimator object(IE: LogisticRegression(), SVC(), KNeighborsClassifier(), etc)\n",
    "* param_grid: a dictionary mapping parameter names to lists of values to search over. We'll see an example below\n",
    "* scoring: a string. See the valid inputs here: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "* n_jobs: number of jobs to run in parallel to speed up the computation. GridSearchCV can take a long time if you have a lot of different parameter combinations to try out. So it's important to set this parameter if your laptop/desktop has a lot of cores that you want to take advantage of.\n",
    "* cv: int, number of cross validation folds to use\n",
    "* refit: boolean, if true, at the end of the Grid Search, GridSearchCV will refit the entire data with a model using the optimal parameters(you can then access this with the GridSearchCV.best\\_parameters_ attribute).\n",
    "\n",
    "Here's an example of how we'd use it for some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.01}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.127041</td>\n",
       "      <td>0.143394</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.01}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.07565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.186551</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.184486</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.034832</td>\n",
       "      <td>0.040748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.1}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       0.001010         0.001104         0.000000          0.000000    0.01   \n",
       "1       0.001523         0.000942         0.127041          0.143394    0.01   \n",
       "2       0.000837         0.000853         0.000000          0.000000     0.1   \n",
       "\n",
       "  param_penalty                           params  rank_test_score  \\\n",
       "0            l1  {u'penalty': u'l1', u'C': 0.01}                9   \n",
       "1            l2  {u'penalty': u'l2', u'C': 0.01}                8   \n",
       "2            l1   {u'penalty': u'l1', u'C': 0.1}                9   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.000000             0.00000       ...                  0.000000   \n",
       "1           0.092593             0.07565       ...                  0.151261   \n",
       "2           0.000000             0.00000       ...                  0.000000   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.000000           0.000000            0.000000   \n",
       "1            0.186551           0.121739            0.138702   \n",
       "2            0.000000           0.000000            0.000000   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.000000            0.000000      0.000060        0.000048   \n",
       "1           0.180328            0.184486      0.000103        0.000061   \n",
       "2           0.000000            0.000000      0.000037        0.000021   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000000         0.000000  \n",
       "1        0.034832         0.040748  \n",
       "2        0.000000         0.000000  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to use GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def sample_cross_val_param_search():\n",
    "    # parameters to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "    param_grid = {'penalty': penalties, 'C': c_vals}\n",
    "    \n",
    "    # make some fake data\n",
    "    n_samples = 1000\n",
    "    feats = 10\n",
    "    x_train = np.random.random((n_samples, feats))\n",
    "    y_train = (np.random.random(n_samples) > 0.5) # randomly generate True/False labels\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, 'f1', cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return grid\n",
    "grid = sample_cross_val_param_search()\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(\"GridSearchCV results:\")\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best Parameters: {'penalty': 'l1', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print('Best estimator: {}'.format(grid.best_estimator_))\n",
    "print('Best Parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Use GridSearchCV to do the same parameter search you did above. What is the best performing set of parameters? Do they match up with what you had above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) As mentioned earlier, GridSearchCV.best\\_estimator\\_ will contain a model with parameters set to the best performing set of parameters fitted on the entire training set. Use this best estimator on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Random Forests has a [feature importances](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) attribute. Which features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Rerun the classifier with a different scoring metric. Do the feature importances change? Which features are most important now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://etav.github.io/projects/spam_message_classifier_naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
